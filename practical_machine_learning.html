<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="date" content="2015-08-23" />

<title>Practical Machine Learning</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link href="data:text/css;charset=utf-8,body%7Bbackground%2Dcolor%3A%23fff%3Bmargin%3A1em%20auto%3Bmax%2Dwidth%3A700px%3Boverflow%3Avisible%3Bpadding%2Dleft%3A2em%3Bpadding%2Dright%3A2em%3Bfont%2Dfamily%3A%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3Bfont%2Dsize%3A14px%3Bline%2Dheight%3A1%2E35%7D%23header%7Btext%2Dalign%3Acenter%7D%23TOC%7Bclear%3Aboth%3Bmargin%3A0%200%2010px%2010px%3Bpadding%3A4px%3Bwidth%3A400px%3Bborder%3A1px%20solid%20%23CCCCCC%3Bborder%2Dradius%3A5px%3Bbackground%2Dcolor%3A%23f6f6f6%3Bfont%2Dsize%3A13px%3Bline%2Dheight%3A1%2E3%7D%23TOC%20%2Etoctitle%7Bfont%2Dweight%3Abold%3Bfont%2Dsize%3A15px%3Bmargin%2Dleft%3A5px%7D%23TOC%20ul%7Bpadding%2Dleft%3A40px%3Bmargin%2Dleft%3A%2D1%2E5em%3Bmargin%2Dtop%3A5px%3Bmargin%2Dbottom%3A5px%7D%23TOC%20ul%20ul%7Bmargin%2Dleft%3A%2D2em%7D%23TOC%20li%7Bline%2Dheight%3A16px%7Dtable%7Bmargin%3A1em%20auto%3Bborder%2Dwidth%3A1px%3Bborder%2Dcolor%3A%23DDDDDD%3Bborder%2Dstyle%3Aoutset%3Bborder%2Dcollapse%3Acollapse%7Dtable%20th%7Bborder%2Dwidth%3A2px%3Bpadding%3A5px%3Bborder%2Dstyle%3Ainset%7Dtable%20td%7Bborder%2Dwidth%3A1px%3Bborder%2Dstyle%3Ainset%3Bline%2Dheight%3A18px%3Bpadding%3A5px%205px%7Dtable%2C%20table%20th%2C%20table%20td%7Bborder%2Dleft%2Dstyle%3Anone%3Bborder%2Dright%2Dstyle%3Anone%7Dtable%20thead%2C%20table%20tr%2Eeven%7Bbackground%2Dcolor%3A%23f7f7f7%7Dp%7Bmargin%3A0%2E5em%200%7Dblockquote%7Bbackground%2Dcolor%3A%23f6f6f6%3Bpadding%3A0%2E25em%200%2E75em%7Dhr%7Bborder%2Dstyle%3Asolid%3Bborder%3Anone%3Bborder%2Dtop%3A1px%20solid%20%23777%3Bmargin%3A28px%200%7Ddl%7Bmargin%2Dleft%3A0%7Ddl%20dd%7Bmargin%2Dbottom%3A13px%3Bmargin%2Dleft%3A13px%7Ddl%20dt%7Bfont%2Dweight%3Abold%7Dul%7Bmargin%2Dtop%3A0%7Dul%20li%7Blist%2Dstyle%3Acircle%20outside%7Dul%20ul%7Bmargin%2Dbottom%3A0%7Dpre%2C%20code%7Bbackground%2Dcolor%3A%23f7f7f7%3Bborder%2Dradius%3A3px%3Bcolor%3A%23333%7Dpre%7Bwhite%2Dspace%3Apre%2Dwrap%3Bborder%2Dradius%3A3px%3Bmargin%3A5px%200px%2010px%200px%3Bpadding%3A10px%7Dpre%3Anot%28%5Bclass%5D%29%7Bbackground%2Dcolor%3A%23f7f7f7%7Dcode%7Bfont%2Dfamily%3AConsolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3Bfont%2Dsize%3A85%25%7Dp%20%3E%20code%2C%20li%20%3E%20code%7Bpadding%3A2px%200px%7Ddiv%2Efigure%7Btext%2Dalign%3Acenter%7Dimg%7Bbackground%2Dcolor%3A%23FFFFFF%3Bpadding%3A2px%3Bborder%3A1px%20solid%20%23DDDDDD%3Bborder%2Dradius%3A3px%3Bborder%3A1px%20solid%20%23CCCCCC%3Bmargin%3A0%205px%7Dh1%7Bmargin%2Dtop%3A0%3Bfont%2Dsize%3A35px%3Bline%2Dheight%3A40px%7Dh2%7Bborder%2Dbottom%3A4px%20solid%20%23f7f7f7%3Bpadding%2Dtop%3A10px%3Bpadding%2Dbottom%3A2px%3Bfont%2Dsize%3A145%25%7Dh3%7Bborder%2Dbottom%3A2px%20solid%20%23f7f7f7%3Bpadding%2Dtop%3A10px%3Bfont%2Dsize%3A120%25%7Dh4%7Bborder%2Dbottom%3A1px%20solid%20%23f7f7f7%3Bmargin%2Dleft%3A8px%3Bfont%2Dsize%3A105%25%7Dh5%2C%20h6%7Bborder%2Dbottom%3A1px%20solid%20%23ccc%3Bfont%2Dsize%3A105%25%7Da%7Bcolor%3A%230033dd%3Btext%2Ddecoration%3Anone%7Da%3Ahover%7Bcolor%3A%236666ff%7Da%3Avisited%7Bcolor%3A%23800080%7Da%3Avisited%3Ahover%7Bcolor%3A%23BB00BB%7Da%5Bhref%5E%3D%22http%3A%22%5D%7Btext%2Ddecoration%3Aunderline%7Da%5Bhref%5E%3D%22https%3A%22%5D%7Btext%2Ddecoration%3Aunderline%7Dcode%20%3E%20span%2Ekw%7Bcolor%3A%23555%3Bfont%2Dweight%3Abold%7Dcode%20%3E%20span%2Edt%7Bcolor%3A%23902000%7Dcode%20%3E%20span%2Edv%7Bcolor%3A%2340a070%7Dcode%20%3E%20span%2Ebn%7Bcolor%3A%23d14%7Dcode%20%3E%20span%2Efl%7Bcolor%3A%23d14%7Dcode%20%3E%20span%2Ech%7Bcolor%3A%23d14%7Dcode%20%3E%20span%2Est%7Bcolor%3A%23d14%7Dcode%20%3E%20span%2Eco%7Bcolor%3A%23888888%3Bfont%2Dstyle%3Aitalic%7Dcode%20%3E%20span%2Eot%7Bcolor%3A%23007020%7Dcode%20%3E%20span%2Eal%7Bcolor%3A%23ff0000%3Bfont%2Dweight%3Abold%7Dcode%20%3E%20span%2Efu%7Bcolor%3A%23900%3Bfont%2Dweight%3Abold%7Dcode%20%3E%20span%2Eer%7Bcolor%3A%23a61717%3Bbackground%2Dcolor%3A%23e3d2d2%7D" rel="stylesheet" type="text/css" />

</head>

<body>



<div id="header">
<h1 class="title">Practical Machine Learning</h1>
<h4 class="date"><em>2015-08-23</em></h4>
</div>


<p>The purpose of this project is to build a machine learning model to make a prediction on data collected from accelerators worn by participants performing various activities. The participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. These ways are classified as:A, B, C, D, and E. The accelerate measured belt, forearm, arm, and dumbbell motions.The dataset was obtained from this source: <a href="http://groupware.les.inf.puc-rio.br/har" class="uri">http://groupware.les.inf.puc-rio.br/har</a> . Further explanation of the experiment can be found in the section on the Weight Lifting Exercise Dataset.</p>
<div id="obtaining-the-data" class="section level2">
<h2>Obtaining the Data</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#load appropriate libraries</span>
<span class="kw">library</span>(RCurl)</code></pre></div>
<pre><code>## Loading required package: bitops</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)</code></pre></div>
<pre><code>## Loading required package: lattice
## Loading required package: ggplot2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(randomForest)</code></pre></div>
<pre><code>## randomForest 4.6-10
## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># download data if it hasn't been downloaded already</span>
if(!<span class="kw">file.exists</span>(<span class="st">&quot;pml-training.csv&quot;</span>)){
   <span class="kw">download.file</span>(<span class="dt">url=</span><span class="st">&quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv&quot;</span>,
              <span class="dt">destfile=</span><span class="st">&quot;pml-training.csv&quot;</span>)
}
if(!<span class="kw">file.exists</span>(<span class="st">&quot;pml-testing.csv&quot;</span>)){
    <span class="kw">download.file</span>(<span class="dt">url=</span><span class="st">&quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv&quot;</span>,
              <span class="dt">destfile=</span><span class="st">&quot;pml-testing.csv&quot;</span>)
}

<span class="co">#load the data into R</span>
pmlTrain&lt;-<span class="kw">read.csv</span>(<span class="dt">file=</span> <span class="st">&quot;pml-training.csv&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;,&quot;</span>)
pmlTest&lt;-<span class="kw">read.csv</span>(<span class="dt">file=</span> <span class="st">&quot;pml-testing.csv&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;,&quot;</span>)

<span class="co"># look at the dimensions of the raw data</span>
<span class="kw">dim</span>(pmlTrain)</code></pre></div>
<pre><code>## [1] 19622   160</code></pre>
</div>
<div id="data-cleaning" class="section level2">
<h2>Data Cleaning</h2>
<p>There are 19,622 observations with 160 variables which gives us over 3 million data points. The data is messy and will have to be cleaned: many columns have multiple ‘NA’ values, and not every variable appears to have a meaningful contribution.</p>
<p>The first step is to eliminate columns with a large number of NA values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># find which columns are more than half full of 'NA's and eliminate them</span>
temp&lt;-pmlTrain[,-<span class="kw">which</span>(<span class="kw">apply</span>(<span class="kw">is.na</span>(pmlTrain), <span class="dv">2</span>, sum)&gt;(<span class="fl">0.5</span>*<span class="kw">nrow</span>(pmlTrain)))]
<span class="kw">dim</span>(temp)</code></pre></div>
<pre><code>## [1] 19622    93</code></pre>
<p>The data set is down to 93 variables. We want the data that addresses dumbbell, arm, forearm, and belt readings, so grab those first. The timestamp is unlikely to help, neither are the skewness, window or kurtosis readings for the remaining variables. Lastly, retain only quantitative data, then reattach the “classe” variable (the variable the model is designed to predict).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#grab dumbbell, arm, forearm, and belt data</span>
temp &lt;-<span class="st"> </span>temp[, <span class="kw">grepl</span>(<span class="st">&quot;arm|forearm|belt|dumbbell&quot;</span>, <span class="kw">colnames</span>(temp))]

<span class="co"># look for keywords using regular expressions and eliminate those columns</span>
temp &lt;-<span class="st"> </span>temp[, !<span class="kw">grepl</span>(<span class="st">&quot;window|kurtosis|^X|skewness|timestamp&quot;</span>, <span class="kw">colnames</span>(temp))]

<span class="co">#select only the remaining columns that are quantitative</span>
temp &lt;-<span class="st"> </span>temp[, <span class="kw">sapply</span>(temp, is.numeric)]

<span class="co"># don't forget to add the 'classe' variable back on! It's essential to building the model</span>
temp$classe&lt;-pmlTrain$classe
<span class="kw">dim</span>(temp)</code></pre></div>
<pre><code>## [1] 19622    53</code></pre>
<p>Now there are 53 variables to feed into the machine learning algorithm, or 52 not including “classe”. This should improve the speed in which the model is built.</p>
</div>
<div id="building-a-model" class="section level2">
<h2>Building a model</h2>
<p>Most training sets use 60% of the available data to build a model, so we’ll use that same proportion.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">123</span>)
inTrain &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(temp$classe, <span class="dt">p=</span><span class="fl">0.6</span>, <span class="dt">list=</span><span class="ot">FALSE</span>)

<span class="co">#create this training set from 60% of the observations</span>
training&lt;-<span class="st"> </span>temp[inTrain, ]
<span class="co">#create this testing set for cross validation later</span>
testing &lt;-<span class="st"> </span>temp[-inTrain, ]</code></pre></div>
<p>Building the model takes a long time, and without proper setting calibration, it can take hours. To avoid unnecessary wasted time while “knitting” this report, use a saved version of the model if it already exists from a previous run.</p>
<p>We’ll use the Random Forest method with 10-fold cross validation and 151 trees. I would prefer to use a higher number of trees but building a random forest from a data set this large will have a huge runtime. The first time I “trained” a random forest model on the cleaned training set using default parameters, the model was still being calculated over an hour later. I believe this compromise is reasonable, which the results confirmed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Building the model takes a long time. If a saved model was created earlier, us it</span>
if (<span class="kw">file.exists</span>(<span class="st">&quot;modFit1.Rds&quot;</span>))
    {
      modFit&lt;-<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;modFit1.Rds&quot;</span>)
} else {
  modFit &lt;-<span class="st"> </span><span class="kw">train</span>(training$classe~., <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>, <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number=</span><span class="dv">10</span>), 
                <span class="dt">data=</span>training, <span class="dt">ntrees=</span><span class="dv">151</span>)
  <span class="kw">saveRDS</span>(modFit, <span class="st">&quot;modFit1.Rds&quot;</span>)
}
modFit</code></pre></div>
<pre><code>## Random Forest 
## 
## 11776 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 10599, 10598, 10599, 10598, 10598, 10598, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
##    2    0.9890456  0.9861418  0.002315315  0.002930185
##   27    0.9891302  0.9862501  0.001870082  0.002364927
##   52    0.9853091  0.9814168  0.002002594  0.002534577
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 27.</code></pre>
</div>
<div id="cross-validation-and-accuracy" class="section level2">
<h2>Cross Validation and Accuracy</h2>
<p>Now that the model is built, we compare to the test set (the 40% of the data set aside for this purpose).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confusionMatrix</span>(testing$classe, <span class="kw">predict</span>(modFit,testing))</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 2230    2    0    0    0
##          B   11 1503    4    0    0
##          C    1    9 1345   13    0
##          D    0    0   16 1269    1
##          E    0    0    5    5 1432
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9915          
##                  95% CI : (0.9892, 0.9934)
##     No Information Rate : 0.2858          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9892          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9946   0.9927   0.9818   0.9860   0.9993
## Specificity            0.9996   0.9976   0.9964   0.9974   0.9984
## Pos Pred Value         0.9991   0.9901   0.9832   0.9868   0.9931
## Neg Pred Value         0.9979   0.9983   0.9961   0.9973   0.9998
## Prevalence             0.2858   0.1930   0.1746   0.1640   0.1826
## Detection Rate         0.2842   0.1916   0.1714   0.1617   0.1825
## Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838
## Balanced Accuracy      0.9971   0.9952   0.9891   0.9917   0.9989</code></pre>
<p>The model appears to be highly accurate with only a few cases mispredicted. Specifically, there were 11 cases of A miscategorized as B, 16 cases of C to D, and 13 cases of D to C. However, these are only a few among over a thousand. The accuracy reported by the confusionMatrix function as performed on the testing data is 99.15%. This is a very high level of accuracy leaving only a 0.85% out of sample error. This level of accuracy is so high that it initially made me concerned about overfitting, but remember that this data set is outside the training set so it really demonstrates how well the prediction model worked.</p>
<div id="results" class="section level3">
<h3>Results</h3>
<p>Now, we use the model on the test data (not to be confused with the data set aside for cross validation)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">final&lt;-<span class="kw">predict</span>(modFit, pmlTest)
final</code></pre></div>
<pre><code>##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E</code></pre>
<p>These predictions were submitted and confirmed to be correct.</p>
</div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
